# 2026-02-16 - Data Architecture Decisions

## Current Issues Identified
1. Mixed data sources (markdown tables with analysis content bleeding together)
2. Stray table rows in analysis sections from holdings table
3. No schema validation leading to silent failures
4. Reactive bug fixes instead of proper architecture

## Architectural Best Practices (Agreed Upon)

### 1. Schema-First Design
- Define JSON schemas before writing any code
- Validate all data against schemas
- Fail fast with explicit errors, not silent returns

### 2. Separation of Concerns (3 Layers)
- **Presentation Layer** (dashboard.html): Display only, no business logic
- **Business Logic Layer** (server.py): Coordinate, transform, no file I/O
- **Data Layer** (data/ folder): Storage, validation, clean returns

### 3. DRY Principle
- One source of truth per data type
- No duplicate data between files
- Holdings and analyses completely separate

### 4. Configuration Over Code
- Environment variables for paths
- Configurable defaults
- Easy to run on different machines

### 5. Phased Migration (Not Big Bang)
- Phase 1: Schema definitions
- Phase 2: Migration scripts
- Phase 3: Dual support (JSON + fallback)
- Phase 4: Parallel testing on port 8081
- Phase 5: Switch port 8080
- Phase 6: Cleanup after 30 days

### 6. Test-Driven Development
- Tests before implementation
- Validation tests for all schemas
- Catch regressions early

## Proposed Data Structure

```
portfolio/
├── schemas/                    # JSON schema definitions
│   ├── holdings.schema.json
│   ├── analysis.schema.json
│   ├── earnings.schema.json
│   ├── schedule.schema.json
│   ├── ideas.schema.json
│   └── corporate.schema.json
├── data/                       # All data files
│   ├── holdings.json          # All positions (ONE file)
│   ├── earnings.json          # All earnings (ONE file)
│   ├── schedule.json          # All events (ONE file)
│   ├── ideas.json             # All ideas (ONE file)
│   ├── corporate.json         # Team structure (ONE file)
│   └── analyses/              # DIRECTORY (independent items)
│       ├── RKT.json
│       ├── LDI.json
│       ├── CRM.json
│       ├── HOOD.json
│       └── SOFI.json
```

## Migration Process
1. **Port 8080**: Keep old markdown running (production)
2. **Port 8081**: Test new JSON implementation
3. Compare outputs between ports
4. Switch only after validation
5. 30-day monitoring period
6. Final cleanup

## Why Separate Analyses?
Each stock analysis is independent:
- Can add/remove/update one without touching others
- No cross-dependencies
- Easier to maintain long-term
- Parallel development possible

## Key Insight
Holdings = grouped data (all positions together)
Analyses = independent data (each stock separate)

## Next Steps
1. Create JSON schemas
2. Build data layer module
3. Write validation tests
4. Implement migration
5. Test on port 8081
6. Switch after approval

## Files Already Created
- `portfolio/analyses/RKT.json` (clean)
- `portfolio/analyses/LDI.json` (clean)
- Both validated and working

## Lessons Learned
- Markdown parsing is fragile
- JSON provides structure and validation
- Schema-first prevents data corruption
- Phased migration reduces risk
- Always have a rollback plan

---

## Afternoon Session - Ideas Pipeline Complete & DeepSeek Migration Plan

### Smart Conversation-to-Idea Capture (Idea-006) - DONE ✅
**All 3 phases completed:**
1. ✅ Phase 1: Trigger detection system (trigger_monitor.py)
2. ✅ Phase 2: Auto-categorization refinement (90% accuracy)
3. ✅ Phase 3: Full chat integration (message_processor.py)

**How it works:** Say "turn that into an idea" → auto-captures with category, effort, context, analysis.

### DeepSeek Migration (Idea-011) - APPROVED
**Target architecture:**
- **DeepSeek V3** → PRIMARY for coding (94% cheaper)
- **Kimi K2.5** → BACKUP for complex reasoning (the "brain")
- **Approach:** Parallel testing to avoid breaking system

**Cost comparison:**
- DeepSeek: $0.14/M input, $0.28/M output (~$1.46/month)
- Kimi: $0.60/M input, $2.50/M output (~$9.62/month)
- **Savings:** ~85% with DeepSeek as primary

**Blocker:** Need API key from https://platform.deepseek.com

### Root Cause Analysis
**Issue wasn't the LLM** - was architecture discipline:
- Coding before clear data contracts
- Mixed concerns (parsing in endpoints)
- Batch changes without testing
- Reactive vs. proactive fixes

**Solution:** TDS-first, strict testing, proper separation of concerns.

### Action Items
- [ ] Get DeepSeek API key
- [ ] Configure parallel model setup
- [ ] Test for 24 hours
- [ ] Switch DeepSeek to primary

**Added to Ideas Pipeline:** Feb 16, 2026
